# Pipeline de Dados

## DescriÃ§Ã£o

Este projeto tem como objetivo realizar a ingestÃ£o, transformaÃ§Ã£o, validaÃ§Ã£o e carga de dados da base pÃºblica do governo brasileiro (SAEB - 9Âº Ano) utilizando ferramentas modernas de orquestraÃ§Ã£o e processamento de dados. O pipeline Ã© composto por dois fluxos principais: Backfill (processamento completo de dados) e Incremental (processamento apenas de dados novos).

### Fonte dos Dados 

Base: basedosdados.br_inep_saeb.aluno_ef_9ano
FrequÃªncia de AtualizaÃ§Ã£o dos Dados: Bienal (a cada 2 anos)

## Arquitetura do Projeto
O projeto Ã© orquestrado com Apache Airflow e utiliza o Docker para garantir portabilidade e fÃ¡cil deploy. O processamento de dados Ã© realizado com PySpark e a validaÃ§Ã£o Ã© feita com Great Expectations.

### Principais Componentes
Airflow: OrquestraÃ§Ã£o das pipelines.
Docker & Docker Compose: ContainerizaÃ§Ã£o dos serviÃ§os.
PySpark: Processamento distribuÃ­do de dados.
Great Expectations: ValidaÃ§Ã£o da qualidade dos dados.
Google BigQuery: Armazenamento dos dados tratados.

### ğŸ—ƒï¸ Estrutura de Pastas

```
.
â”œâ”€â”€ README.md                   # DocumentaÃ§Ã£o do projeto
â”œâ”€â”€ docker-compose.yaml         # ConfiguraÃ§Ã£o dos containers
â”œâ”€â”€ dockerfile                  # DefiniÃ§Ã£o da imagem Docker
â”œâ”€â”€ requirements.txt            # DependÃªncias do projeto
â”œâ”€â”€ dags/                       # DAGs do Airflow
â”‚   â”œâ”€â”€ backfill_dag.py         # Pipeline de carga completa (Backfill)
â”‚   â””â”€â”€ incremental_dag.py      # Pipeline de ingestÃ£o incremental
â”œâ”€â”€ utils/                      # FunÃ§Ãµes auxiliares
â”‚   â”œâ”€â”€ etl.py                  # LÃ³gica de extraÃ§Ã£o, transformaÃ§Ã£o e carga
â”‚   â””â”€â”€ data_validation.py      # ValidaÃ§Ãµes de qualidade com Great Expectations
â”œâ”€â”€ great_expectations/         # ConfiguraÃ§Ãµes e checkpoints de validaÃ§Ã£o
â”‚   â”œâ”€â”€ checkpoints             # Checkpoints para validaÃ§Ã£o de dados
â”‚   â”œâ”€â”€ expectations            # Regras e expectativas de dados
â”‚   â”œâ”€â”€ great_expectations.yml  # Arquivo de configuraÃ§Ã£o do GE
â”‚   â””â”€â”€ profilers               # Perfis de dados gerados
â”œâ”€â”€ infra/                      # Infraestrutura como cÃ³digo (Terraform)
â”‚   â”œâ”€â”€ main.tf                 # CriaÃ§Ã£o dos recursos (BigQuery, etc.)
â”‚   â”œâ”€â”€ outputs.tf              # SaÃ­das dos recursos criados
â”‚   â”œâ”€â”€ provider.tf             # Provedor do Terraform (GCP)
â”‚   â”œâ”€â”€ terraform.tfvars        # VariÃ¡veis definidas pelo usuÃ¡rio
â”‚   â”œâ”€â”€ variables.tf            # DefiniÃ§Ã£o de variÃ¡veis
â”‚   â”œâ”€â”€ terraform.tfstate       # Estado atual da infraestrutura
â”‚   â””â”€â”€ terraform.tfstate.backup# Backup do estado da infraestrutura

```

### Como Executar o Projeto

Para executar este projeto, Ã© necessÃ¡rio ter os seguintes prÃ©-requisitos instalados em seu computador:

* [Python](https://www.python.org/downloads/) (recomendado: versÃ£o 3.8 ou superior)

* [Docker](https://www.docker.com/get-started/) (necessÃ¡rio para orquestraÃ§Ã£o com Airflow e execuÃ§Ã£o dos containers)

* [Terraform](https://developer.hashicorp.com/terraform/tutorials) (alternativa para nÃ£o ter que criar a infraestrutura de armazenamento manualmente)

* [Great Expactation](https://maikpaixao.medium.com/data-quality-with-great-expectation-in-python-0908b179f615)(ferramenta que auxilia na qualidade de validaÃ§Ã£o dos dados)

### Passo a Passo 

Com todo as ferramentas instaladas em seu ambiente, agora Ã© sÃ³ seguir o passo a passo a seguir 

```
git clone git@github.com:nicole-malaquias/data_ingestion_pipeline.git
cd data_ingestion_pipeline
```

### Levantando sua Infra 

Para que seja possivel fazer inserÃ§Ã£o de dados  Ã© preciso provisionar um recurso de armazenamento na cloud, por motivos de custo, esse projeto usa o google bigquery e asseguir terÃ¡ os passos auxiliando a configura-lo para seu projeto. 

#### IntegraÃ§Ã£o com o BigQuery

Este projeto utiliza o **[BigQuery](https://support.google.com/cloud/answer/9113366?hl=pt-BR#:~:text=O%20BigQuery%20%C3%A9%20um%20servi%C3%A7o,administrador%20de%20banco%20de%20dados.)** como soluÃ§Ã£o de armazenamento de dados. Para integrar o projeto ao data warehouse, Ã© necessÃ¡rio ativar uma **conta de serviÃ§o** no console da **Google Cloud Platform (GCP)**.

### ğŸ”‘ Recursos Ãºteis

- ğŸ› ï¸ **Ativar interaÃ§Ãµes externas no GCP:**  
  Siga o guia detalhado para habilitar seu console da GCP e permitir interaÃ§Ãµes externas:  
  ğŸ‘‰ [Como ativar o serviÃ§o de transferÃªncia no BigQuery](https://cloud.google.com/bigquery/docs/enable-transfer-service?hl=pt-br)

- ğŸ—ï¸ **Criar uma conta de serviÃ§o e gerar credenciais:**  
  Aprenda a criar uma conta de serviÃ§o e gerar a credencial necessÃ¡ria para autenticaÃ§Ã£o com o Google:  
  ğŸ‘‰ [Guia para criar uma conta de serviÃ§o no GCP](https://support.site24x7.com/portal/en/kb/articles/how-to-create-a-service-account-in-gcp-console)

- ğŸ› ï¸ **Criando Projeto dentro do Console GCP:** 
  Apos os passos acima por fim crie um projeto com o nome "letrus". 
  ğŸ‘‰ [Como de como criar o projeto](https://medium.com/@camila-marquess/criando-um-projeto-no-dbt-utilizando-o-bigquery-c49fc8375aa2#:~:text=Cria%C3%A7%C3%A3o%20do%20Projeto%20no%20GCP&text=D%C3%AA%20um%20nome%20ao%20seu,clique%20em%20continue%20e%20Done.)


Como esse projeto serÃ¡ executado apenas para fins demostrativos de como fazer o fluxo de extraÃ§Ã£o , transformaÃ§Ã£o e carregamento, adicione o token extraido na raiz do projeto. 

#### Terraform apply 

No seu terminal vÃ¡ atÃ© a pasta infra, certifique-se que esteja dentro dela, entÃ£o execute o terraform para que crie o mesma estrutura de armazenamento em seu console gcp. 

```
terraform applay
``` 

#### Configure e inicie o Docker:

Certifique-se de que o Docker estÃ¡ rodando em sua mÃ¡quina.
Para instalar o Docker, siga o guia oficial: InstalaÃ§Ã£o do Docker
Inicie os containers com o comando:

```
docker-compose up -d
```

#### Executando o Airflow 

Assim que todo os containers estiverem *up* acesse [localhost:8000](http://localhost:8080/)

Use as credenciais padrÃ£o (se aplicÃ¡vel):
- UsuÃ¡rio: airflow
- Senha: airflow

#### Execute os DAGs:

No Airflow, ative e execute os DAGs desejados:

**backfill**: Esta DAG tem como objetivo realizar a carga inicial de dados, preenchendo o banco de dados com todas as informaÃ§Ãµes atualmente disponÃ­veis na base pÃºblica. Deve ser executada apenas uma vez para popular o histÃ³rico de dados.


**incremental_fill**: Esta DAG realiza a ingestÃ£o incremental, verificando se hÃ¡ dados mais recentes na base pÃºblica do governo. Caso sejam identificados novos registros, a DAG realiza a ingestÃ£o apenas desses dados, garantindo que o banco de dados permaneÃ§a atualizado sem redundÃ¢ncias.







